---
title: "AI-naloga-unfinished"
output:
  pdf_document: default
  html_notebook: default
---

## Obdelava podatkov in dodajanje atributov

Na začetku uvozimo podatke z gumbom v RStudio imenovanem "Import Dataset" in sicer izberemo opcijo "From Text(base)". Nato ustvarimo novo delovno spremenljivko "data", da ostane prvotna spremenljivka nespremenjena. 


```{r}
dataSem1 <- read.csv("../../AI-naloga/dataSem1.txt", stringsAsFactors=TRUE)

data <- dataSem1
data$regija <- as.factor(data$regija)
data$namembnost <- as.factor(data$namembnost)
data$norm_poraba <- as.factor(data$norm_poraba)

head(data)
```

Atribute "regija", "namembnost" in "norm_poraba" faktoriziramo, da programski jezik razbere zalogo vrednosti. V tabeli zgoraj je prikazanih prvih nekaj vrstic. 

```{r}
data$datum <- as.Date(data$datum)
head(data)

```

Lastnost atributa "datum" nastavimo na <date>.

```{r}
library(lubridate)

newData <- week(data$datum)

data$teden <- newData

newNewData <- month(data$datum)
data$mesec <- newNewData

head(data)
```

V tem delu dodamo nova atributa "teden" in "mesec" za lažje nadaljno delo. Za pomoč uporabimo knjižnico "lubridate".

```{r}
library(chron)

x <- is.weekend(data$datum)
data$vikend <- x
```

Zgornja koda prikazuje kako dodamo atribut "vikend", ki ima vrednost TRUE, če je dan sobota ali nedelja, oziroma FALSE, če je delovni dan. Pomagamo si s knjižnico "chron".

```{r Ustvarjen atribut UNIX}
data$UNIX <- as.numeric(as.POSIXct(paste(data$datum, data$ura, sep=" ")))
```

Ustvarimo splošen atribut za čas, ki nam omogoča lažje sortiranje.

```{r}
a <- order(data$stavba, data$ura, data$UNIX)
sortedDF <- data[a,]
sortedDF
```

Dataframe sortiramo po številki stavbe, uri meritve in UNIX času.

```{r}
vect = vector()
starTemp = 0
uraMeritve = 0
for (row in 1:nrow(sortedDF))
{
  if(uraMeritve == sortedDF[row, "ura"]) {
    vect <- c(vect, starTemp)
  }
  else {
    uraMeritve = sortedDF[row, "ura"]
    vect <- c(vect, sortedDF[row, "temp_zraka"])
  }
  starTemp = sortedDF[row, "temp_zraka"]
}

```

```{r}
sortedDF$norm_poraba <- factor(sortedDF$norm_poraba, levels=c("ZELONIZKA", "NIZKA", "SREDNJA", "VISOKA", "ZELOVISOKA"))

sortedDF$prev_temp <- vect
head(sortedDF)
```

S kodo zgornjih dveh odlomkov vsaki meritvi v sortirani matriki poiščemo meritev temperature prejšnjega dne in ji le to dopišemo v poseben stolpec z atributi.

```{r}
vect = vector()
starTemp = 0
uraMeritve = 0
substract = 0
beginning = 0
stevec = 0
for (row in 1:nrow(sortedDF))
{
  if(beginning < row-7)
    stevec = row-7
  else
    stevec = beginning
  if(uraMeritve == sortedDF[row, "ura"]) {
    starTemp = 0
    #r <- which(sortedDF[, "stavba"] == sortedDF[row, "stavba"] & sortedDF[, "ura"] == sortedDF[row, "ura"] & sortedDF[, "datum"] == sortedDF[row, "datum"] - substract)
    for (i in stevec:(row-1))
      starTemp = starTemp + sortedDF[i, "temp_zraka"]
    avgTemp = starTemp / (row - stevec)
    vect <- c(vect, avgTemp)
  }
  else {
    beginning = row
    uraMeritve = sortedDF[row, "ura"]
    vect <- c(vect, sortedDF[row, "temp_zraka"])
  }
  starTemp = sortedDF[row, "temp_zraka"]
}
```

```{r}
sortedDF$temp_teden <- vect
head(sortedDF)
```

Z zanko se sprehodimo po vsaki meritvi v matriki in izračunamo povprečje največ zadnjih 7 meritev za isto stavbo ob isti uri. Povprečno temperaturo zapišemo v stolpec z imenom "temp_teden".


## Večinski razred

```{r}
plot(sortedDF$norm_poraba, main = "Porazdelitev primerov glede na normirano stopnjo porabe")
```

```{r}
summary(sortedDF$norm_poraba)
```
Večinski parameter je vrednost "SREDNJA", ki predstavlja 76.435 vrednosti od 206.785, kar je 0,3696 od celotne zaloge elementov.


## Odločitveno drevo

```{r}
library(dplyr)
library(rpart)

dt_list <- list()

for(m in 1:11) {
  tempDF <- filter(sortedDF, mesec <= m)
  
  drops <- c("poraba")
  tempDF <- tempDF[ , !(names(tempDF) %in% drops)]
  dt_list[[m]] <- rpart(norm_poraba ~ ., data = tempDF)
}
```
Za izluščenje le posameznega meseca se uporabi knjižnica "dplyr", kjer s pomočjo fukcije "filter" izluščimo le podatke določenega meseca.
Zgoraj so narejena drevesa za vseh 11 mesecev razen decembra, ki se samo testira.


```{r}
library(rpart.plot)

for(m in dt_list)
  rpart.plot(m)
```



```{r}
i = 1
predicted <- list()
for (dt in dt_list) {
  tempDF <- filter(sortedDF, mesec == i+1)
  predicted[[i]] <- predict(dt, tempDF, type = "class")
  i = i + 1
}
```

### Analiza odločitvenega drevesa
```{r}
# Tabela zmot
cat("Pravilnost rezultatov \n")
for(i in 1:11){
  cat("Mesec =", i+1, " ->  ")
  q <- filter(sortedDF, mesec == i+1)$norm_poraba == predicted[[i]]
  cat(mean(q))
  #tab <- table(predicted[[i]],filter(sortedDF, mesec == i+1)$norm_poraba)
  cat(" +/-", sd(q)/sqrt(length(q)), "\n")
}
```


```{r}
cat("Senzitivnost za 'SREDNJA'\n")
for(i in 1:11){
  cat("Mesec =", i+1, " ->  ")
  tab <- table(filter(sortedDF, mesec == i+1)$norm_poraba, predicted[[i]])
  cat(tab["SREDNJA", "SREDNJA"]/sum(tab["SREDNJA",]), "\n")
}
```



```{r}
cat("Specifičnost za 'SREDNJA'\n")
for(i in 1:11){
  cat("Mesec =", i+1, " ->  ")
  tab <- table(filter(sortedDF, mesec == i+1)$norm_poraba, predicted[[i]])
  cat((sum(tab)-sum(tab["SREDNJA",])-sum(tab[,"SREDNJA"])+tab["SREDNJA","SREDNJA"])/(sum(tab)-sum(tab[,"SREDNJA"])), "\n")
}
```


```{r}
cat("Preciznost za 'SREDNJA'\n")
for(i in 1:11){
  cat("Mesec =", i+1, " ->  ")
  tab <- table(filter(sortedDF, mesec == i+1)$norm_poraba, predicted[[i]])
  cat(tab["SREDNJA", "SREDNJA"]/sum(tab[,"SREDNJA"]), "\n")
}
```

```{r}
i = 1
predictedMat <- list()
for (dt in dt_list) {
  tempDF <- filter(sortedDF, mesec == i+1)
  predictedMat[[i]] <- predict(dt, tempDF, type = "prob")
  i = i + 1
}
col.order <- c("ZELONIZKA", "NIZKA", "SREDNJA", "VISOKA", "ZELOVISOKA")
head(predictedMat[[1]][,col.order])
```

```{r}
library(nnet)

observedMat <- list()
for (i in 1:11) {
  observedMat[[i]] <- class.ind(factor(filter(sortedDF, mesec == i+1)$norm_poraba, levels=c("ZELONIZKA", "NIZKA", "SREDNJA", "VISOKA", "ZELOVISOKA")))
}
head(observedMat[[1]])
```


```{r}
print("Barier's test", quote = F)
for(i in 1:11){
  cat("Mesec =", i+1, " ->  ")
  bar <- sum((observedMat[[i]] - predictedMat[[i]]) ^ 2) / nrow(predictedMat[[i]])
  cat(bar, "\n")
}
```

```{r}
p0 <- table(sortedDF$norm_poraba)/nrow(sortedDF)
cat("Trivialni model\n")
for(i in 1:11){
  cat("Mesec =", i+1, " ->  ")
  p0Mat <- matrix(rep(p0, times=nrow(observedMat[[i]])), nrow = nrow(observedMat[[i]]), byrow = T)
  colnames(p0Mat) <- names(p0)
  bar <- sum((observedMat[[i]] - p0Mat) ^ 2) / nrow(p0Mat)
  cat(bar, "\n")
}

```
> Tale trivialen model je kao boljša metoda kot pa najino drevo. To je treba še enkrat preverit, ker se ne sklada z rezultatom zgoraj. - Najbrž zaradi tega, ker so rezultati v p0Mat povprečje od vseh vrednosti, namesto primerljive učne vrednosti, kot so v posameznih učnih množicah, na katerih potem izdelamo odločitveno drevo

```{r Informacijska vsebina odgovora}
inf.score <- function(trainClass, testClass, predMat) {
  result <- 0
  priors <- table(trainClass) / length(trainClass)
  
  for (i in 1:nrow(predMat)) {
    p.prior <- priors[testClass[i]]
    p.posterior <- predMat[i, testClass[i]]
    
    if(p.posterior >= p.prior) {
      result <- result - log2(p.prior) + log2(p.posterior)
    }
    else
      result <- result + log2(1-p.prior) - log2(1-p.posterior)
  }
  
  result/nrow(predMat)
}
```

```{r}
cat("Informacijska vsebina odgovora\n")
for (i in 1:11) {
  cat("Mesec =", i+1, " ->  ")
  cat(inf.score(filter(sortedDF, mesec <= i)$norm_poraba, filter(sortedDF, mesec == i+1)$norm_poraba, predictedMat[[i]]), "\n")
}
```

```{r}
cat("Informacijska vsebina trivialnega modela\n")
for (i in 1:11) {
  cat("Mesec =", i+1, " ->  ")
  p0Mat <- matrix(rep(p0, times=nrow(filter(sortedDF, mesec == i+1))), nrow = nrow(filter(sortedDF, mesec == i+1)), byrow = T)
  cat(inf.score(filter(sortedDF, mesec <= i)$norm_poraba, filter(sortedDF, mesec == i+1)$norm_poraba, p0Mat), "\n")
}
```
> Spet podoben problem, kot zgoraj pri trivialnem modelu. Vse vrednosti bi morale biti 0, ampak najbrž niso zaradi napačnega izračuna povprečne porazdelitve v p0 in potem generirani matriki p0Mat











## Testni chunk

```{r}

```














#################################################################################
#################################################################################
#                                REGRESIJA                                      #
#################################################################################
#################################################################################


# DEKLARACIJA FUNKCIJ ZA OCENJEVANJE #

```{r}
mae <- function(obs, pred)
{
	mean(abs(obs - pred))
}


mse <- function(obs, pred)
{
	mean((obs - pred)^2)
}

rmae <- function(obs, pred, mean.val) 
{  
	sum(abs(obs - pred)) / sum(abs(obs - mean.val))
}

rmse <- function(obs, pred, mean.val) 
{  
	sum((obs - pred)^2)/sum((obs - mean.val)^2)
}
```

Najprej se deklarira funkcije, ki se bojo uporabljale za ocenjevanje modelov.

MAE -> srednja absolutna napaka

MSE -> srednja kvadratna napaka

RMAE -> relativna srednja absolutna napak

RMSE -> relativna srednja kvadratna napaka



##############################################
# MODELI NAUČENI NA CELOTNI MNOŽICI PODATKOV #
##############################################

```{r}
dataForRegression <- sortedDF
set.seed(0)
sel <- sample(1:nrow(dataForRegression), as.integer(nrow(dataForRegression) * 0.7), F)
train <- dataForRegression[sel,]
test <- dataForRegression[-sel,]
```

Podatki za regresijo se preberejo iz v naprej pripravljenega dataframe-a "sortedDF". Nastavi se učne in testne podatke, ki se jih razdeli v razmerju 3:7.


# LINEARNA REGRESIJA #

```{r}
model <- lm(poraba ~ ., train)
predicted <- predict(model, test)
observed <- test$poraba
plot(observed)
points(predicted, col="red")

mae(observed, predicted)
mse(observed, predicted)
sqrt(mse(observed, predicted))
```

Linearna regresija nam za vse podatke vrne:

SREDNJA ABSOLUTNA NAPAKA = 82.04917

SREDNJA KVADRATNA NAPAKA (korenjena, da je v enotah atributa "poraba") = 137.2372


# DODATNI TEST S TRIVIALNIM MODELOM (GLEDE NA LINEARNEGA) #

```{r}
meanVal <- mean(train$poraba)
meanVal

predTrivial <- rep(meanVal, nrow(test))
mae(observed, predTrivial)
mse(observed, predTrivial)
sqrt(mse(observed, predTrivial))

```
Izračunamo povprečno koncentracijo porabe v testni množici(meanVal). Nato izračunamo še srednjo absolutno in
srednjo kvadratno napako za trivialen model, ki ga zgradimo s funkcijo "rep(meanVal, nrow(test)).

Opazimo, da ima naš linearni model manjši obe napaki, kar pomeni, da se je linearni model nekaj naučil in je
boljši od trivialnega modela.


```{r}
rmae(observed, predicted, mean(train$poraba))
rmse(observed, predicted, mean(train$poraba))
```

S pomočjo relativnih mer ocenimo naš linearni model v primerjavi s trivialnim. Zgornja vrnjena številka prikazuje
relativno srednjo absolutno napako, spodnja pa relativno srednjo kvadratno napako. Ker so vrednosti manjše od 1, to 
pomeni, da je linearni model bojši od trivialnega.


# regresijsko drevo #

```{r}
rt.model <- rpart(poraba ~ ., data=train)
rpart.plot(rt.model)
predicted <- predict(rt.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```

Regresijsko drevo nam za vse podatke vrne:

SREDNJA ABSOLUTNA NAPAKA = 53.48211

RELATIVNA SREDNJA ABSOLUTNA NAPAKA = 0.3533899 
(Bližje kot je številka, ki jo vrne RMAE, 0, boljši je model.)


# nakljucni gozd #

```{r}
library(randomForest)

rf.model <- randomForest(poraba ~ ., train, ntree=10, norm.votes=FALSE)
predicted <- predict(rf.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))

```

Naključni gozd nam za vse podatke vrne:

SREDNJA ABSOLUTNA NAPAKA = 27.48959

RELATIVNA SREDNJA ABSOLUTNA NAPAKA = 0.181641




# IZBIRANJE ATRIBUTOV Z NAJVEČJIM VPLIVOM NA CILJNO/ODVISNO SPREMENLJIVKO #

```{r}
dataForRegression = subset(sortedDF, select = -c(norm_poraba, teden, UNIX, prev_temp, temp_teden))

dataForRegressionJan <- filter(dataForRegression, mesec == 1)
for (i in 1:ncol(dataForRegressionJan))
  dataForRegressionJan[,i] <- as.factor(dataForRegressionJan[,i])
```

Podatke za regresijo se vzame iz že urejenih podatkov "sortedDF" brez "norm_poraba" in se jih faktorizira. 

```{r}
library(CORElearn)

dataForRegressionJan = subset(dataForRegressionJan, select = -c(mesec))
sort(attrEval(poraba ~ ., dataForRegressionJan, "InfGain"),  decreasing = TRUE)
```
Preveri se kateri atributi največ pripomorejo pri napovedovanju ciljne spremenljivke. Izbere se najboljše, torej
tiste, ki imajo najvišjo oceno. Zaradi prevelike količine podatkov se preveri za vsak mesec posebej(npr. zgoraj za januar).

```{r}
dataForRegression = subset(dataForRegression, select = -c(mesec, vikend, padavine, regija, ura, namembnost, oblacnost))
summary(dataForRegression)
```

Množica podatkov se zmanjša na 9 atributov (poleg atributa "poraba"), ki najbolj pripomorejo k napovedovanju atributa "poraba".


# LINEARNA REGRESIJA #   

```{r}
set.seed(0)
sel <- sample(1:nrow(dataForRegression), as.integer(nrow(dataForRegression) * 0.7), F)
train <- dataForRegression[sel,]
test <- dataForRegression[-sel,]

model <- lm(poraba ~ ., train)
predicted <- predict(model, test)
observed <- test$poraba
plot(observed)
points(predicted, col="red")

mae(observed, predicted)
mse(observed, predicted)
sqrt(mse(observed, predicted))
```

Nastavimo seed na 0, da je ob vsakem zagonu enaka naključnost.Vsi podakti se razdelijo na testno in 
učno množico v razmerju 3:7. Model se naredi glede na opazovano spremenljivko "poraba".Graf za te
podatke je prikazan zgoraj.V tem primeru je linearna regresija narejena za vse mesece.

SREDNJA ABSOLUTNA NAPAKA = 99.04282

SREDNJA KVADRATNA NAPAKA (korenjena, da je v enotah atributa "poraba") = 180.3948


# regresijsko drevo #

```{r}
rt.model <- rpart(poraba ~ ., data=train)
rpart.plot(rt.model)
predicted <- predict(rt.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```

Regresijsko drevo nam za vse podatke vrne:

SREDNJA ABSOLUTNA NAPAKA = 63.07831

RELATIVNA SREDNJA ABSOLUTNA NAPAKA = 0.416798 
(Bližje kot je številka, ki jo vrne RMAE, 0, boljši je model.)


# nakljucni gozd #

```{r}
rf.model <- randomForest(poraba ~ ., train, ntree=10, norm.votes=FALSE)
predicted <- predict(rf.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```

Naključni gozd nam za vse podatke vrne:

SREDNJA ABSOLUTNA NAPAKA = 

RELATIVNA SREDNJA ABSOLUTNA NAPAKA = 




#############################################
# EVALVACIJA MODELOV PO MESECIH (REGRESIJA) #
#############################################

```{r}
dataForRegression = subset(sortedDF, select = -c(norm_poraba, teden, UNIX, prev_temp, temp_teden, vikend, padavine, regija, ura, namembnost, oblacnost))
summary(dataForRegression)
```
Ponovno se nastavi spremenljivka "dataForRegression". 

```{r}
dataForRegressionJan <- filter(dataForRegression, mesec == 1)
dataForRegressionJan = subset(dataForRegressionJan, select = -c(mesec))
```

Izlušči se podatke samo za januar.

```{r}
dataForRegressionFeb <- filter(dataForRegression, mesec == 2)
dataForRegressionFeb = subset(dataForRegressionFeb, select = -c(mesec))
```

Izlušči se podatke samo za februar.

```{r}
dataForRegressionMar <- filter(dataForRegression, mesec == 3)
dataForRegressionMar = subset(dataForRegressionMar, select = -c(mesec))
```

Izlušči se podatke samo za marec.

```{r}
dataForRegressionApr <- filter(dataForRegression, mesec == 4)
dataForRegressionApr = subset(dataForRegressionApr, select = -c(mesec))
```

Izlušči se podatke samo za april.

```{r}
dataForRegressionMay <- filter(dataForRegression, mesec == 5)
dataForRegressionMay = subset(dataForRegressionMay, select = -c(mesec))
```

Izlušči se podatke samo za maj.

```{r}
dataForRegressionJun <- filter(dataForRegression, mesec == 6)
dataForRegressionJun = subset(dataForRegressionJun, select = -c(mesec))
```

Izlušči se podatke samo za junij.

```{r}
dataForRegressionJul <- filter(dataForRegression, mesec == 7)
dataForRegressionJul = subset(dataForRegressionJul, select = -c(mesec))
```

Izlušči se podatke samo za julij.

```{r}
dataForRegressionAug <- filter(dataForRegression, mesec == 8)
dataForRegressionAug = subset(dataForRegressionAug, select = -c(mesec))
```

Izlušči se podatke samo za august.

```{r}
dataForRegressionSep <- filter(dataForRegression, mesec == 9)
dataForRegressionSep = subset(dataForRegressionSep, select = -c(mesec))
```

Izlušči se podatke samo za september.

```{r}
dataForRegressionOct <- filter(dataForRegression, mesec == 10)
dataForRegressionOct = subset(dataForRegressionOct, select = -c(mesec))
```

Izlušči se podatke samo za oktober.

```{r}
dataForRegressionNov <- filter(dataForRegression, mesec == 11)
dataForRegressionNov = subset(dataForRegressionNov, select = -c(mesec))
```

Izlušči se podatke samo za november.

```{r}
dataForRegressionDec <- filter(dataForRegression, mesec == 12)
dataForRegressionDec = subset(dataForRegressionDec, select = -c(mesec))
```

Izlušči se podatke samo za december.




###########
# FEBRUAR #
###########

```{r}
dataForTrain <- dataForRegressionJan
dataForTest <- dataForRegressionFeb

train <- dataForTrain
test <- dataForTest
```

Nastavi se učne in testne podatke. V tem primeru bomo testirali januarske na februarskih.

# LINEARNA REGRESIJA #
```{r}
model <- lm(poraba ~ ., train)
predicted <- predict(model, test)
observed <- test$poraba
plot(observed)
points(predicted, col="red")

mae(observed, predicted)
mse(observed, predicted)
sqrt(mse(observed, predicted))
```

# regresijsko drevo #
```{r}
rt.model <- rpart(poraba ~ ., data=train)
rpart.plot(rt.model)
predicted <- predict(rt.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```

# nakljucni gozd #
```{r}
rf.model <- randomForest(poraba ~ ., train, ntree=10, norm.votes=FALSE)
predicted <- predict(rf.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```



#########
# MAREC #
#########


```{r}
dataForTrain <- rbind(dataForRegressionJan, dataForRegressionFeb)
dataForTest <- dataForRegressionMar

train <- dataForTrain
test <- dataForTest
```
Nastavi se učne in testne podatke. V tem primeru bomo testirali vse predhodnje podatke + februar na marčevskih.

# LINEARNA REGRESIJA #
```{r}
model <- lm(poraba ~ ., train)
predicted <- predict(model, test)
observed <- test$poraba
plot(observed)
points(predicted, col="red")

mae(observed, predicted)
mse(observed, predicted)
sqrt(mse(observed, predicted))
```

# regresijsko drevo #
```{r}
rt.model <- rpart(poraba ~ ., data=train)
rpart.plot(rt.model)
predicted <- predict(rt.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```

# nakljucni gozd #
```{r}
rf.model <- randomForest(poraba ~ ., train, ntree=10, norm.votes=FALSE)
predicted <- predict(rf.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```



#########
# APRIL #
#########


```{r}
dataForTrain <- rbind(dataForTrain, dataForRegressionMar)
dataForTest <- dataForRegressionApr

train <- dataForTrain
test <- dataForTest
```
Nastavi se učne in testne podatke. V tem primeru bomo testirali vse predhodnje podatke + marec na aprilskih.

# LINEARNA REGRESIJA #
```{r}
model <- lm(poraba ~ ., train)
predicted <- predict(model, test)
observed <- test$poraba
plot(observed)
points(predicted, col="red")

mae(observed, predicted)
mse(observed, predicted)
sqrt(mse(observed, predicted))
```

# regresijsko drevo #
```{r}
rt.model <- rpart(poraba ~ ., data=train)
rpart.plot(rt.model)
predicted <- predict(rt.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```

# nakljucni gozd #
```{r}
rf.model <- randomForest(poraba ~ ., train, ntree=10, norm.votes=FALSE)
predicted <- predict(rf.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```



#######
# MAJ #
#######


```{r}
dataForTrain <- rbind(dataForTrain, dataForRegressionApr)
dataForTest <- dataForRegressionMay

train <- dataForTrain
test <- dataForTest
```
Nastavi se učne in testne podatke. V tem primeru bomo testirali vse predhodnje podatke + april na majevih.

# LINEARNA REGRESIJA #
```{r}
model <- lm(poraba ~ ., train)
predicted <- predict(model, test)
observed <- test$poraba
plot(observed)
points(predicted, col="red")

mae(observed, predicted)
mse(observed, predicted)
sqrt(mse(observed, predicted))
```

# regresijsko drevo #
```{r}
rt.model <- rpart(poraba ~ ., data=train)
rpart.plot(rt.model)
predicted <- predict(rt.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```

# nakljucni gozd #
```{r}
rf.model <- randomForest(poraba ~ ., train, ntree=10, norm.votes=FALSE)
predicted <- predict(rf.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```




#########
# JUNIJ #
#########


```{r}
dataForTrain <- rbind(dataForTrain, dataForRegressionMay)
dataForTest <- dataForRegressionJun

train <- dataForTrain
test <- dataForTest
```
Nastavi se učne in testne podatke. V tem primeru bomo testirali vse predhodnje podatke + maj na junijskih.

# LINEARNA REGRESIJA #
```{r}
model <- lm(poraba ~ ., train)
predicted <- predict(model, test)
observed <- test$poraba
plot(observed)
points(predicted, col="red")

mae(observed, predicted)
mse(observed, predicted)
sqrt(mse(observed, predicted))
```

# regresijsko drevo #
```{r}
rt.model <- rpart(poraba ~ ., data=train)
rpart.plot(rt.model)
predicted <- predict(rt.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```

# nakljucni gozd #
```{r}
rf.model <- randomForest(poraba ~ ., train, ntree=10, norm.votes=FALSE)
predicted <- predict(rf.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```



#########
# JULIJ #
#########


```{r}
dataForTrain <- rbind(dataForTrain, dataForRegressionJun)
dataForTest <- dataForRegressionJul

train <- dataForTrain
test <- dataForTest
```
Nastavi se učne in testne podatke. V tem primeru bomo testirali vse predhodnje podatke + junij na julijskih.

# LINEARNA REGRESIJA #
```{r}
model <- lm(poraba ~ ., train)
predicted <- predict(model, test)
observed <- test$poraba
plot(observed)
points(predicted, col="red")

mae(observed, predicted)
mse(observed, predicted)
sqrt(mse(observed, predicted))
```

# regresijsko drevo #
```{r}
rt.model <- rpart(poraba ~ ., data=train)
rpart.plot(rt.model)
predicted <- predict(rt.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```

# nakljucni gozd #
```{r}
rf.model <- randomForest(poraba ~ ., train, ntree=10, norm.votes=FALSE)
predicted <- predict(rf.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```



##########
# AUGUST #
##########


```{r}
dataForTrain <- rbind(dataForTrain, dataForRegressionJul)
dataForTest <- dataForRegressionAug

train <- dataForTrain
test <- dataForTest
```
Nastavi se učne in testne podatke. V tem primeru bomo testirali vse predhodnje podatke + julij na augustovih.

# LINEARNA REGRESIJA #
```{r}
model <- lm(poraba ~ ., train)
predicted <- predict(model, test)
observed <- test$poraba
plot(observed)
points(predicted, col="red")

mae(observed, predicted)
mse(observed, predicted)
sqrt(mse(observed, predicted))
```

# regresijsko drevo #
```{r}
rt.model <- rpart(poraba ~ ., data=train)
rpart.plot(rt.model)
predicted <- predict(rt.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```

# nakljucni gozd #
```{r}
rf.model <- randomForest(poraba ~ ., train, ntree=10, norm.votes=FALSE)
predicted <- predict(rf.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```



#############
# SEPTEMBER #
#############


```{r}
dataForTrain <- rbind(dataForTrain, dataForRegressionAug)
dataForTest <- dataForRegressionSep

train <- dataForTrain
test <- dataForTest
```
Nastavi se učne in testne podatke. V tem primeru bomo testirali vse predhodnje podatke + august na septembrskih.

# LINEARNA REGRESIJA #
```{r}
model <- lm(poraba ~ ., train)
predicted <- predict(model, test)
observed <- test$poraba
plot(observed)
points(predicted, col="red")

mae(observed, predicted)
mse(observed, predicted)
sqrt(mse(observed, predicted))
```

# regresijsko drevo #
```{r}
rt.model <- rpart(poraba ~ ., data=train)
rpart.plot(rt.model)
predicted <- predict(rt.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```

# nakljucni gozd #
```{r}
rf.model <- randomForest(poraba ~ ., train, ntree=10, norm.votes=FALSE)
predicted <- predict(rf.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```



###########
# OKTOBER #
###########


```{r}
dataForTrain <- rbind(dataForTrain, dataForRegressionSep)
dataForTest <- dataForRegressionOct

train <- dataForTrain
test <- dataForTest
```
Nastavi se učne in testne podatke. V tem primeru bomo testirali vse predhodnje podatke + september na oktobrskih.

# LINEARNA REGRESIJA #
```{r}
model <- lm(poraba ~ ., train)
predicted <- predict(model, test)
observed <- test$poraba
plot(observed)
points(predicted, col="red")

mae(observed, predicted)
mse(observed, predicted)
sqrt(mse(observed, predicted))
```

# regresijsko drevo #
```{r}
rt.model <- rpart(poraba ~ ., data=train)
rpart.plot(rt.model)
predicted <- predict(rt.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```

# nakljucni gozd #
```{r}
rf.model <- randomForest(poraba ~ ., train, ntree=10, norm.votes=FALSE)
predicted <- predict(rf.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```



############
# NOVEMBER #
############


```{r}
dataForTrain <- rbind(dataForTrain, dataForRegressionOct)
dataForTest <- dataForRegressionNov

train <- dataForTrain
test <- dataForTest
```
Nastavi se učne in testne podatke. V tem primeru bomo testirali vse predhodnje podatke + oktober na novemberskih.

# LINEARNA REGRESIJA #
```{r}
model <- lm(poraba ~ ., train)
predicted <- predict(model, test)
observed <- test$poraba
plot(observed)
points(predicted, col="red")

mae(observed, predicted)
mse(observed, predicted)
sqrt(mse(observed, predicted))
```

# regresijsko drevo #
```{r}
rt.model <- rpart(poraba ~ ., data=train)
rpart.plot(rt.model)
predicted <- predict(rt.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```

# nakljucni gozd #
```{r}
rf.model <- randomForest(poraba ~ ., train, ntree=10, norm.votes=FALSE)
predicted <- predict(rf.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```



############
# DECEMBER #
############


```{r}
dataForTrain <- rbind(dataForTrain, dataForRegressionNov)
dataForTest <- dataForRegressionDec

train <- dataForTrain
test <- dataForTest
```
Nastavi se učne in testne podatke. V tem primeru bomo testirali vse predhodnje podatke + novemberske na decembrskih.

# LINEARNA REGRESIJA #
```{r}
model <- lm(poraba ~ ., train)
predicted <- predict(model, test)
observed <- test$poraba
plot(observed)
points(predicted, col="red")

mae(observed, predicted)
mse(observed, predicted)
sqrt(mse(observed, predicted))
```

# regresijsko drevo #
```{r}
rt.model <- rpart(poraba ~ ., data=train)
rpart.plot(rt.model)
predicted <- predict(rt.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```

# nakljucni gozd #
```{r}
rf.model <- randomForest(poraba ~ ., train, ntree=10, norm.votes=FALSE)
predicted <- predict(rf.model, test)
mae(test$poraba, predicted)
rmae(test$poraba, predicted, mean(train$poraba))
```

